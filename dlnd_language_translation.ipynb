{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 语言翻译\n",
    "\n",
    "在此项目中，你将了解神经网络机器翻译这一领域。你将用由英语和法语语句组成的数据集，训练一个序列到序列模型（sequence to sequence model），该模型能够将新的英语句子翻译成法语。\n",
    "\n",
    "## 获取数据\n",
    "\n",
    "因为将整个英语语言内容翻译成法语需要大量训练时间，所以我们提供了一小部分的英语语料库。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "source_path = 'data/small_vocab_en'\n",
    "target_path = 'data/small_vocab_fr'\n",
    "source_text = helper.load_data(source_path)\n",
    "target_text = helper.load_data(target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 探索数据\n",
    "\n",
    "研究 view_sentence_range，查看并熟悉该数据的不同部分。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 227\n",
      "Number of sentences: 137861\n",
      "Average number of words in a sentence: 13.225277634719028\n",
      "\n",
      "English sentences 0 to 10:\n",
      "new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "the united states is usually chilly during july , and it is usually freezing in november .\n",
      "california is usually quiet during march , and it is usually hot in june .\n",
      "the united states is sometimes mild during june , and it is cold in september .\n",
      "your least liked fruit is the grape , but my least liked is the apple .\n",
      "his favorite fruit is the orange , but my favorite is the grape .\n",
      "paris is relaxing during december , but it is usually chilly in july .\n",
      "new jersey is busy during spring , and it is never hot in march .\n",
      "our least liked fruit is the lemon , but my least liked is the grape .\n",
      "the united states is sometimes busy during january , and it is sometimes warm in november .\n",
      "\n",
      "French sentences 0 to 10:\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
      "votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n",
      "son fruit préféré est l'orange , mais mon préféré est le raisin .\n",
      "paris est relaxant en décembre , mais il est généralement froid en juillet .\n",
      "new jersey est occupé au printemps , et il est jamais chaude en mars .\n",
      "notre fruit est moins aimé le citron , mais mon moins aimé est le raisin .\n",
      "les états-unis est parfois occupé en janvier , et il est parfois chaud en novembre .\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 10)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in source_text.split()})))\n",
    "\n",
    "sentences = source_text.split('\\n')\n",
    "word_counts = [len(sentence.split()) for sentence in sentences]\n",
    "print('Number of sentences: {}'.format(len(sentences)))\n",
    "print('Average number of words in a sentence: {}'.format(np.average(word_counts)))\n",
    "\n",
    "print()\n",
    "print('English sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(source_text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))\n",
    "print()\n",
    "print('French sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(target_text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现预处理函数\n",
    "\n",
    "### 文本到单词 id\n",
    "\n",
    "和之前的 RNN 一样，你必须首先将文本转换为数字，这样计算机才能读懂。在函数 `text_to_ids()` 中，你需要将单词中的 `source_text` 和 `target_text` 转为 id。但是，你需要在 `target_text` 中每个句子的末尾，添加 `<EOS>` 单词 id。这样可以帮助神经网络预测句子应该在什么地方结束。\n",
    "\n",
    "\n",
    "你可以通过以下代码获取  `<EOS> ` 单词ID：\n",
    "\n",
    "```python\n",
    "target_vocab_to_int['<EOS>']\n",
    "```\n",
    "\n",
    "你可以使用 `source_vocab_to_int` 和 `target_vocab_to_int` 获得其他单词 id。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Convert source and target text to proper word ids\n",
    "    :param source_text: String that contains all the source text.\n",
    "    :param target_text: String that contains all the target text.\n",
    "    :param source_vocab_to_int: Dictionary to go from the source words to an id\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :return: A tuple of lists (source_id_text, target_id_text)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    source_id_text = []\n",
    "    target_id_text = []\n",
    "    for sentence in source_text.split('\\n'):\n",
    "        source_id_text.append([source_vocab_to_int[word] for word in sentence.split()])\n",
    "    for sentence in target_text.split('\\n'):                 \n",
    "        target_id_text.append([target_vocab_to_int[word] for word in sentence.split()] + [target_vocab_to_int['<EOS>']])\n",
    "    return (source_id_text, target_id_text)\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_text_to_ids(text_to_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预处理所有数据并保存\n",
    "\n",
    "运行以下代码单元，预处理所有数据，并保存到文件中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "helper.preprocess_and_save_data(source_path, target_path, text_to_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检查点\n",
    "\n",
    "这是你的第一个检查点。如果你什么时候决定再回到该记事本，或需要重新启动该记事本，可以从这里继续。预处理的数据已保存到磁盘上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import helper\n",
    "\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 检查 TensorFlow 版本，确认可访问 GPU\n",
    "\n",
    "这一检查步骤，可以确保你使用的是正确版本的 TensorFlow，并且能够访问 GPU。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.0.1\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) in [LooseVersion('1.0.0'), LooseVersion('1.0.1')], 'This project requires TensorFlow version 1.0  You are using {}'.format(tf.__version__)\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建神经网络\n",
    "\n",
    "你将通过实现以下函数，构建出要构建一个序列到序列模型所需的组件：\n",
    "\n",
    "- `model_inputs`\n",
    "- `process_decoding_input`\n",
    "- `encoding_layer`\n",
    "- `decoding_layer_train`\n",
    "- `decoding_layer_infer`\n",
    "- `decoding_layer`\n",
    "- `seq2seq_model`\n",
    "\n",
    "### 输入\n",
    "\n",
    "实现 `model_inputs()` 函数，为神经网络创建 TF 占位符。该函数应该创建以下占位符：\n",
    "\n",
    "- 名为 “input” 的输入文本占位符，并使用 TF Placeholder 名称参数（等级（Rank）为 2）。\n",
    "- 目标占位符（等级为 2）。\n",
    "- 学习速率占位符（等级为 0）。\n",
    "- 名为 “keep_prob” 的保留率占位符，并使用 TF Placeholder 名称参数（等级为 0）。\n",
    "\n",
    "在以下元祖（tuple）中返回占位符：（输入、目标、学习速率、保留率）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def model_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate, keep probability)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    inputs = tf.placeholder(tf.int32, shape=(None, None), name='input')\n",
    "    targets = tf.placeholder(tf.int32, shape=(None, None), name='target')\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    return (inputs, targets, learning_rate, keep_prob)\n",
    "  \n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_model_inputs(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理解码输入\n",
    "\n",
    "使用 TensorFlow 实现 `process_decoding_input`，以便删掉 `target_data` 中每个批次的最后一个单词 ID，并将 GO ID 放到每个批次的开头。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def process_decoding_input(target_data, target_vocab_to_int, batch_size):\n",
    "    \"\"\"\n",
    "    Preprocess target data for dencoding\n",
    "    :param target_data: Target Placehoder\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :param batch_size: Batch Size\n",
    "    :return: Preprocessed target data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function'\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    decoded = tf.concat([tf.fill([batch_size, 1], target_vocab_to_int['<GO>']), ending], 1)\n",
    "    return decoded\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_process_decoding_input(process_decoding_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编码\n",
    "\n",
    "实现 `encoding_layer()`，以使用 [`tf.nn.dynamic_rnn()`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn) 创建编码器 RNN 层级。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob):\n",
    "    \"\"\"\n",
    "    Create encoding layer\n",
    "    :param rnn_inputs: Inputs for the RNN\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: RNN state\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size, state_is_tuple=True)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop]* num_layers)\n",
    "    outputs, state =tf.nn.dynamic_rnn(cell,rnn_inputs,dtype=tf.float32)\n",
    "   \n",
    "    return state\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_encoding_layer(encoding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解码 - 训练\n",
    "\n",
    "使用 [`tf.contrib.seq2seq.simple_decoder_fn_train()`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/seq2seq/simple_decoder_fn_train) 和 [`tf.contrib.seq2seq.dynamic_rnn_decoder()`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/seq2seq/dynamic_rnn_decoder) 创建训练分对数（training logits）。将 `output_fn` 应用到 [`tf.contrib.seq2seq.dynamic_rnn_decoder()`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/seq2seq/dynamic_rnn_decoder) 输出上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope,\n",
    "                         output_fn, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a decoding layer for training\n",
    "    :param encoder_state: Encoder State\n",
    "    :param dec_cell: Decoder RNN Cell\n",
    "    :param dec_embed_input: Decoder embedded input\n",
    "    :param sequence_length: Sequence Length\n",
    "    :param decoding_scope: TenorFlow Variable Scope for decoding\n",
    "    :param output_fn: Function to apply the output layer\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: Train Logits\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    dynamic_fn_train = tf.contrib.seq2seq.simple_decoder_fn_train(encoder_state)\n",
    "    outputs_train, state_train, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, dynamic_fn_train,dec_embed_input,\n",
    "                                                                           sequence_length,scope=decoding_scope)\n",
    "    outputs_logit= output_fn(outputs_train)\n",
    "    outputs_logit= tf.nn.dropout(outputs_logit, keep_prob)\n",
    "    return outputs_logit\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_decoding_layer_train(decoding_layer_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解码 - 推论\n",
    "\n",
    "使用 [`tf.contrib.seq2seq.simple_decoder_fn_inference()`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/seq2seq/simple_decoder_fn_inference) 和 [`tf.contrib.seq2seq.dynamic_rnn_decoder()`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/seq2seq/dynamic_rnn_decoder) 创建推论分对数（inference logits）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id,\n",
    "                         maximum_length, vocab_size, decoding_scope, output_fn, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a decoding layer for inference\n",
    "    :param encoder_state: Encoder state\n",
    "    :param dec_cell: Decoder RNN Cell\n",
    "    :param dec_embeddings: Decoder embeddings\n",
    "    :param start_of_sequence_id: GO ID\n",
    "    :param end_of_sequence_id: EOS Id\n",
    "    :param maximum_length: The maximum allowed time steps to decode\n",
    "    :param vocab_size: Size of vocabulary\n",
    "    :param decoding_scope: TensorFlow Variable Scope for decoding\n",
    "    :param output_fn: Function to apply the output layer\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: Inference Logits\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    decoder_fn = tf.contrib.seq2seq.simple_decoder_fn_inference(output_fn, encoder_state, dec_embeddings, start_of_sequence_id, \n",
    "                                                              end_of_sequence_id, maximum_length, vocab_size)    \n",
    "    outputs,_,_ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, decoder_fn, scope=decoding_scope)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_decoding_layer_infer(decoding_layer_infer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建解码层级\n",
    "\n",
    "实现 `decoding_layer()` 以创建解码器 RNN 层级。\n",
    "\n",
    "- 使用 `rnn_size` 和 `num_layers` 创建解码 RNN 单元。\n",
    "- 使用 [`lambda`](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions) 创建输出函数，将输入，也就是分对数转换为类分对数（class logits）。\n",
    "- 使用 `decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope, output_fn, keep_prob)` 函数获取训练分对数。\n",
    "- 使用 `decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id, maximum_length, vocab_size, decoding_scope, output_fn, keep_prob)` 函数获取推论分对数。\n",
    "\n",
    "注意：你将需要使用 [tf.variable_scope](https://www.tensorflow.org/api_docs/python/tf/variable_scope) 在训练和推论分对数间分享变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def decoding_layer(dec_embed_input, dec_embeddings, encoder_state, vocab_size, sequence_length, rnn_size,\n",
    "                   num_layers, target_vocab_to_int, keep_prob):\n",
    "    \"\"\"\n",
    "    Create decoding layer\n",
    "    :param dec_embed_input: Decoder embedded input\n",
    "    :param dec_embeddings: Decoder embeddings\n",
    "    :param encoder_state: The encoded state\n",
    "    :param vocab_size: Size of vocabulary\n",
    "    :param sequence_length: Sequence Length\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: Tuple of (Training Logits, Inference Logits)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "    dec_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    \n",
    "    with tf.variable_scope('decoding') as decoding_scope:\n",
    "        output_fn = lambda x: tf.contrib.layers.fully_connected(x, vocab_size, None, scope=decoding_scope)\n",
    "        \n",
    "        train_logits = decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length,\n",
    "                                            decoding_scope, output_fn, keep_prob)\n",
    "        \n",
    "    with tf.variable_scope('decoding', reuse=True) as decoding_scope:\n",
    "        infer_logits = decoding_layer_infer(encoder_state, dec_cell, dec_embeddings,\n",
    "                                            source_vocab_to_int['<GO>'], source_vocab_to_int['<EOS>'],\n",
    "                                            sequence_length, vocab_size, decoding_scope, output_fn, keep_prob)\n",
    "\n",
    "    return train_logits, infer_logits\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_decoding_layer(decoding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建神经网络\n",
    "\n",
    "应用你在上方实现的函数，以：\n",
    "\n",
    "- 向编码器的输入数据应用嵌入。\n",
    "- 使用 `encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob)` 编码输入。\n",
    "- 使用 `process_decoding_input(target_data, target_vocab_to_int, batch_size)` 函数处理目标数据。\n",
    "- 向解码器的目标数据应用嵌入。\n",
    "- 使用 `decoding_layer(dec_embed_input, dec_embeddings, encoder_state, vocab_size, sequence_length, rnn_size, num_layers, target_vocab_to_int, keep_prob)` 解码编码的输入数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size, sequence_length, source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size, rnn_size, num_layers, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Build the Sequence-to-Sequence part of the neural network\n",
    "    :param input_data: Input placeholder\n",
    "    :param target_data: Target placeholder\n",
    "    :param keep_prob: Dropout keep probability placeholder\n",
    "    :param batch_size: Batch Size\n",
    "    :param sequence_length: Sequence Length\n",
    "    :param source_vocab_size: Source vocabulary size\n",
    "    :param target_vocab_size: Target vocabulary size\n",
    "    :param enc_embedding_size: Decoder embedding size\n",
    "    :param dec_embedding_size: Encoder embedding size\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :return: Tuple of (Training Logits, Inference Logits)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    enc_embed_input = tf.contrib.layers.embed_sequence(input_data, source_vocab_size, enc_embedding_size)\n",
    "    enc_state = encoding_layer(enc_embed_input, rnn_size, num_layers, keep_prob=keep_prob)\n",
    "    target_data = process_decoding_input(target_data, target_vocab_to_int, batch_size)\n",
    "    dec_embed = tf.Variable(tf.random_uniform([target_vocab_size, dec_embedding_size]))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embed, target_data)\n",
    "\n",
    "    dec_layer = decoding_layer(dec_embed_input, dec_embed, enc_state, target_vocab_size, sequence_length,\n",
    "                               rnn_size, num_layers, target_vocab_to_int, keep_prob)\n",
    "    \n",
    "    return dec_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_seq2seq_model(seq2seq_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练神经网络\n",
    "\n",
    "### 超参数\n",
    "\n",
    "调试以下参数：\n",
    "\n",
    "- 将 `epochs` 设为 epoch 次数。\n",
    "- 将 `batch_size` 设为批次大小。\n",
    "- 将 `rnn_size` 设为 RNN 的大小。\n",
    "- 将 `num_layers` 设为层级数量。\n",
    "- 将 `encoding_embedding_size` 设为编码器嵌入大小。\n",
    "- 将 `decoding_embedding_size` 设为解码器嵌入大小\n",
    "- 将 `learning_rate` 设为训练速率。\n",
    "- 将 `keep_probability` 设为丢弃保留率（Dropout keep probability）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "epochs = 7\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "# RNN Size\n",
    "rnn_size = 256\n",
    "# Number of Layers\n",
    "num_layers = 2\n",
    "# Embedding Size\n",
    "encoding_embedding_size = 128\n",
    "decoding_embedding_size = 128\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "# Dropout Keep Probability\n",
    "keep_probability = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建图表\n",
    "\n",
    "使用你实现的神经网络构建图表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_path = 'checkpoints/dev'\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = helper.load_preprocess()\n",
    "max_source_sentence_length = max([len(sentence) for sentence in source_int_text])\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, lr, keep_prob = model_inputs()\n",
    "    sequence_length = tf.placeholder_with_default(max_source_sentence_length, None, name='sequence_length')\n",
    "    input_shape = tf.shape(input_data)\n",
    "    \n",
    "    train_logits, inference_logits = seq2seq_model(\n",
    "        tf.reverse(input_data, [-1]), targets, keep_prob, batch_size, sequence_length, len(source_vocab_to_int), len(target_vocab_to_int),\n",
    "        encoding_embedding_size, decoding_embedding_size, rnn_size, num_layers, target_vocab_to_int)\n",
    "\n",
    "    tf.identity(inference_logits, 'logits')\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            train_logits,\n",
    "            targets,\n",
    "            tf.ones([input_shape[0], sequence_length]))\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练\n",
    "\n",
    "利用预处理的数据训练神经网络。如果很难获得低损失值，请访问我们的论坛，看看其他人是否遇到了相同的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/1077 - Train Accuracy:  0.297, Validation Accuracy:  0.306, Loss:  5.885\n",
      "Epoch   0 Batch    1/1077 - Train Accuracy:  0.221, Validation Accuracy:  0.305, Loss:  5.539\n",
      "Epoch   0 Batch    2/1077 - Train Accuracy:  0.206, Validation Accuracy:  0.305, Loss:  5.174\n",
      "Epoch   0 Batch    3/1077 - Train Accuracy:  0.239, Validation Accuracy:  0.305, Loss:  4.722\n",
      "Epoch   0 Batch    4/1077 - Train Accuracy:  0.229, Validation Accuracy:  0.309, Loss:  4.685\n",
      "Epoch   0 Batch    5/1077 - Train Accuracy:  0.275, Validation Accuracy:  0.319, Loss:  4.421\n",
      "Epoch   0 Batch    6/1077 - Train Accuracy:  0.278, Validation Accuracy:  0.336, Loss:  4.270\n",
      "Epoch   0 Batch    7/1077 - Train Accuracy:  0.261, Validation Accuracy:  0.337, Loss:  4.221\n",
      "Epoch   0 Batch    8/1077 - Train Accuracy:  0.269, Validation Accuracy:  0.337, Loss:  4.121\n",
      "Epoch   0 Batch    9/1077 - Train Accuracy:  0.274, Validation Accuracy:  0.337, Loss:  3.976\n",
      "Epoch   0 Batch   10/1077 - Train Accuracy:  0.240, Validation Accuracy:  0.337, Loss:  4.022\n",
      "Epoch   0 Batch   11/1077 - Train Accuracy:  0.303, Validation Accuracy:  0.343, Loss:  3.792\n",
      "Epoch   0 Batch   12/1077 - Train Accuracy:  0.295, Validation Accuracy:  0.362, Loss:  3.797\n",
      "Epoch   0 Batch   13/1077 - Train Accuracy:  0.351, Validation Accuracy:  0.366, Loss:  3.613\n",
      "Epoch   0 Batch   14/1077 - Train Accuracy:  0.330, Validation Accuracy:  0.366, Loss:  3.574\n",
      "Epoch   0 Batch   15/1077 - Train Accuracy:  0.316, Validation Accuracy:  0.376, Loss:  3.622\n",
      "Epoch   0 Batch   16/1077 - Train Accuracy:  0.337, Validation Accuracy:  0.385, Loss:  3.553\n",
      "Epoch   0 Batch   17/1077 - Train Accuracy:  0.339, Validation Accuracy:  0.385, Loss:  3.469\n",
      "Epoch   0 Batch   18/1077 - Train Accuracy:  0.320, Validation Accuracy:  0.384, Loss:  3.524\n",
      "Epoch   0 Batch   19/1077 - Train Accuracy:  0.333, Validation Accuracy:  0.379, Loss:  3.399\n",
      "Epoch   0 Batch   20/1077 - Train Accuracy:  0.327, Validation Accuracy:  0.388, Loss:  3.391\n",
      "Epoch   0 Batch   21/1077 - Train Accuracy:  0.312, Validation Accuracy:  0.393, Loss:  3.422\n",
      "Epoch   0 Batch   22/1077 - Train Accuracy:  0.318, Validation Accuracy:  0.391, Loss:  3.393\n",
      "Epoch   0 Batch   23/1077 - Train Accuracy:  0.336, Validation Accuracy:  0.403, Loss:  3.382\n",
      "Epoch   0 Batch   24/1077 - Train Accuracy:  0.351, Validation Accuracy:  0.406, Loss:  3.198\n",
      "Epoch   0 Batch   25/1077 - Train Accuracy:  0.361, Validation Accuracy:  0.416, Loss:  3.313\n",
      "Epoch   0 Batch   26/1077 - Train Accuracy:  0.344, Validation Accuracy:  0.415, Loss:  3.303\n",
      "Epoch   0 Batch   27/1077 - Train Accuracy:  0.402, Validation Accuracy:  0.416, Loss:  3.102\n",
      "Epoch   0 Batch   28/1077 - Train Accuracy:  0.379, Validation Accuracy:  0.426, Loss:  3.162\n",
      "Epoch   0 Batch   29/1077 - Train Accuracy:  0.372, Validation Accuracy:  0.418, Loss:  3.124\n",
      "Epoch   0 Batch   30/1077 - Train Accuracy:  0.368, Validation Accuracy:  0.428, Loss:  3.142\n",
      "Epoch   0 Batch   31/1077 - Train Accuracy:  0.381, Validation Accuracy:  0.436, Loss:  3.178\n",
      "Epoch   0 Batch   32/1077 - Train Accuracy:  0.429, Validation Accuracy:  0.435, Loss:  2.983\n",
      "Epoch   0 Batch   33/1077 - Train Accuracy:  0.408, Validation Accuracy:  0.439, Loss:  3.022\n",
      "Epoch   0 Batch   34/1077 - Train Accuracy:  0.395, Validation Accuracy:  0.453, Loss:  3.074\n",
      "Epoch   0 Batch   35/1077 - Train Accuracy:  0.391, Validation Accuracy:  0.443, Loss:  3.022\n",
      "Epoch   0 Batch   36/1077 - Train Accuracy:  0.405, Validation Accuracy:  0.445, Loss:  3.035\n",
      "Epoch   0 Batch   37/1077 - Train Accuracy:  0.400, Validation Accuracy:  0.453, Loss:  3.071\n",
      "Epoch   0 Batch   38/1077 - Train Accuracy:  0.364, Validation Accuracy:  0.453, Loss:  3.189\n",
      "Epoch   0 Batch   39/1077 - Train Accuracy:  0.393, Validation Accuracy:  0.458, Loss:  3.034\n",
      "Epoch   0 Batch   40/1077 - Train Accuracy:  0.409, Validation Accuracy:  0.461, Loss:  2.957\n",
      "Epoch   0 Batch   41/1077 - Train Accuracy:  0.428, Validation Accuracy:  0.460, Loss:  2.912\n",
      "Epoch   0 Batch   42/1077 - Train Accuracy:  0.402, Validation Accuracy:  0.455, Loss:  2.960\n",
      "Epoch   0 Batch   43/1077 - Train Accuracy:  0.368, Validation Accuracy:  0.449, Loss:  3.082\n",
      "Epoch   0 Batch   44/1077 - Train Accuracy:  0.366, Validation Accuracy:  0.463, Loss:  3.186\n",
      "Epoch   0 Batch   45/1077 - Train Accuracy:  0.389, Validation Accuracy:  0.451, Loss:  2.958\n",
      "Epoch   0 Batch   46/1077 - Train Accuracy:  0.396, Validation Accuracy:  0.466, Loss:  3.029\n",
      "Epoch   0 Batch   47/1077 - Train Accuracy:  0.406, Validation Accuracy:  0.457, Loss:  2.898\n",
      "Epoch   0 Batch   48/1077 - Train Accuracy:  0.437, Validation Accuracy:  0.478, Loss:  2.874\n",
      "Epoch   0 Batch   49/1077 - Train Accuracy:  0.413, Validation Accuracy:  0.458, Loss:  2.876\n",
      "Epoch   0 Batch   50/1077 - Train Accuracy:  0.403, Validation Accuracy:  0.469, Loss:  2.961\n",
      "Epoch   0 Batch   51/1077 - Train Accuracy:  0.417, Validation Accuracy:  0.458, Loss:  2.797\n",
      "Epoch   0 Batch   52/1077 - Train Accuracy:  0.410, Validation Accuracy:  0.470, Loss:  3.031\n",
      "Epoch   0 Batch   53/1077 - Train Accuracy:  0.423, Validation Accuracy:  0.471, Loss:  2.837\n",
      "Epoch   0 Batch   54/1077 - Train Accuracy:  0.401, Validation Accuracy:  0.468, Loss:  3.018\n",
      "Epoch   0 Batch   55/1077 - Train Accuracy:  0.443, Validation Accuracy:  0.473, Loss:  2.787\n",
      "Epoch   0 Batch   56/1077 - Train Accuracy:  0.430, Validation Accuracy:  0.488, Loss:  2.854\n",
      "Epoch   0 Batch   57/1077 - Train Accuracy:  0.489, Validation Accuracy:  0.478, Loss:  2.616\n",
      "Epoch   0 Batch   58/1077 - Train Accuracy:  0.432, Validation Accuracy:  0.487, Loss:  2.843\n",
      "Epoch   0 Batch   59/1077 - Train Accuracy:  0.382, Validation Accuracy:  0.483, Loss:  2.890\n",
      "Epoch   0 Batch   60/1077 - Train Accuracy:  0.452, Validation Accuracy:  0.486, Loss:  2.785\n",
      "Epoch   0 Batch   61/1077 - Train Accuracy:  0.428, Validation Accuracy:  0.480, Loss:  2.741\n",
      "Epoch   0 Batch   62/1077 - Train Accuracy:  0.400, Validation Accuracy:  0.487, Loss:  2.907\n",
      "Epoch   0 Batch   63/1077 - Train Accuracy:  0.438, Validation Accuracy:  0.473, Loss:  2.656\n",
      "Epoch   0 Batch   64/1077 - Train Accuracy:  0.432, Validation Accuracy:  0.490, Loss:  2.795\n",
      "Epoch   0 Batch   65/1077 - Train Accuracy:  0.394, Validation Accuracy:  0.466, Loss:  2.903\n",
      "Epoch   0 Batch   66/1077 - Train Accuracy:  0.423, Validation Accuracy:  0.489, Loss:  2.754\n",
      "Epoch   0 Batch   67/1077 - Train Accuracy:  0.433, Validation Accuracy:  0.456, Loss:  2.664\n",
      "Epoch   0 Batch   68/1077 - Train Accuracy:  0.391, Validation Accuracy:  0.475, Loss:  2.901\n",
      "Epoch   0 Batch   69/1077 - Train Accuracy:  0.448, Validation Accuracy:  0.485, Loss:  2.693\n",
      "Epoch   0 Batch   70/1077 - Train Accuracy:  0.415, Validation Accuracy:  0.489, Loss:  2.878\n",
      "Epoch   0 Batch   71/1077 - Train Accuracy:  0.402, Validation Accuracy:  0.472, Loss:  2.768\n",
      "Epoch   0 Batch   72/1077 - Train Accuracy:  0.402, Validation Accuracy:  0.474, Loss:  2.738\n",
      "Epoch   0 Batch   73/1077 - Train Accuracy:  0.425, Validation Accuracy:  0.486, Loss:  2.684\n",
      "Epoch   0 Batch   74/1077 - Train Accuracy:  0.444, Validation Accuracy:  0.486, Loss:  2.615\n",
      "Epoch   0 Batch   75/1077 - Train Accuracy:  0.437, Validation Accuracy:  0.480, Loss:  2.594\n",
      "Epoch   0 Batch   76/1077 - Train Accuracy:  0.414, Validation Accuracy:  0.475, Loss:  2.665\n",
      "Epoch   0 Batch   77/1077 - Train Accuracy:  0.423, Validation Accuracy:  0.495, Loss:  2.704\n",
      "Epoch   0 Batch   78/1077 - Train Accuracy:  0.405, Validation Accuracy:  0.498, Loss:  2.732\n",
      "Epoch   0 Batch   79/1077 - Train Accuracy:  0.407, Validation Accuracy:  0.484, Loss:  2.689\n",
      "Epoch   0 Batch   80/1077 - Train Accuracy:  0.405, Validation Accuracy:  0.478, Loss:  2.664\n",
      "Epoch   0 Batch   81/1077 - Train Accuracy:  0.458, Validation Accuracy:  0.494, Loss:  2.551\n",
      "Epoch   0 Batch   82/1077 - Train Accuracy:  0.502, Validation Accuracy:  0.496, Loss:  2.439\n",
      "Epoch   0 Batch   83/1077 - Train Accuracy:  0.405, Validation Accuracy:  0.486, Loss:  2.706\n",
      "Epoch   0 Batch   84/1077 - Train Accuracy:  0.416, Validation Accuracy:  0.477, Loss:  2.525\n",
      "Epoch   0 Batch   85/1077 - Train Accuracy:  0.456, Validation Accuracy:  0.500, Loss:  2.495\n",
      "Epoch   0 Batch   86/1077 - Train Accuracy:  0.446, Validation Accuracy:  0.502, Loss:  2.604\n",
      "Epoch   0 Batch   87/1077 - Train Accuracy:  0.416, Validation Accuracy:  0.472, Loss:  2.606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch   88/1077 - Train Accuracy:  0.409, Validation Accuracy:  0.472, Loss:  2.572\n",
      "Epoch   0 Batch   89/1077 - Train Accuracy:  0.434, Validation Accuracy:  0.498, Loss:  2.558\n",
      "Epoch   0 Batch   90/1077 - Train Accuracy:  0.412, Validation Accuracy:  0.491, Loss:  2.640\n",
      "Epoch   0 Batch   91/1077 - Train Accuracy:  0.465, Validation Accuracy:  0.480, Loss:  2.330\n",
      "Epoch   0 Batch   92/1077 - Train Accuracy:  0.427, Validation Accuracy:  0.498, Loss:  2.524\n",
      "Epoch   0 Batch   93/1077 - Train Accuracy:  0.440, Validation Accuracy:  0.504, Loss:  2.491\n",
      "Epoch   0 Batch   94/1077 - Train Accuracy:  0.440, Validation Accuracy:  0.494, Loss:  2.475\n",
      "Epoch   0 Batch   95/1077 - Train Accuracy:  0.460, Validation Accuracy:  0.493, Loss:  2.408\n",
      "Epoch   0 Batch   96/1077 - Train Accuracy:  0.450, Validation Accuracy:  0.500, Loss:  2.473\n",
      "Epoch   0 Batch   97/1077 - Train Accuracy:  0.446, Validation Accuracy:  0.495, Loss:  2.446\n",
      "Epoch   0 Batch   98/1077 - Train Accuracy:  0.465, Validation Accuracy:  0.480, Loss:  2.348\n",
      "Epoch   0 Batch   99/1077 - Train Accuracy:  0.404, Validation Accuracy:  0.490, Loss:  2.435\n",
      "Epoch   0 Batch  100/1077 - Train Accuracy:  0.433, Validation Accuracy:  0.493, Loss:  2.408\n",
      "Epoch   0 Batch  101/1077 - Train Accuracy:  0.411, Validation Accuracy:  0.481, Loss:  2.375\n",
      "Epoch   0 Batch  102/1077 - Train Accuracy:  0.476, Validation Accuracy:  0.510, Loss:  2.346\n",
      "Epoch   0 Batch  103/1077 - Train Accuracy:  0.371, Validation Accuracy:  0.484, Loss:  2.562\n",
      "Epoch   0 Batch  104/1077 - Train Accuracy:  0.377, Validation Accuracy:  0.500, Loss:  2.598\n",
      "Epoch   0 Batch  105/1077 - Train Accuracy:  0.457, Validation Accuracy:  0.495, Loss:  2.312\n",
      "Epoch   0 Batch  106/1077 - Train Accuracy:  0.395, Validation Accuracy:  0.460, Loss:  2.464\n",
      "Epoch   0 Batch  107/1077 - Train Accuracy:  0.447, Validation Accuracy:  0.484, Loss:  2.291\n",
      "Epoch   0 Batch  108/1077 - Train Accuracy:  0.499, Validation Accuracy:  0.512, Loss:  2.209\n",
      "Epoch   0 Batch  109/1077 - Train Accuracy:  0.419, Validation Accuracy:  0.476, Loss:  2.389\n",
      "Epoch   0 Batch  110/1077 - Train Accuracy:  0.432, Validation Accuracy:  0.475, Loss:  2.315\n",
      "Epoch   0 Batch  111/1077 - Train Accuracy:  0.455, Validation Accuracy:  0.516, Loss:  2.371\n",
      "Epoch   0 Batch  112/1077 - Train Accuracy:  0.437, Validation Accuracy:  0.502, Loss:  2.399\n",
      "Epoch   0 Batch  113/1077 - Train Accuracy:  0.404, Validation Accuracy:  0.475, Loss:  2.356\n",
      "Epoch   0 Batch  114/1077 - Train Accuracy:  0.429, Validation Accuracy:  0.466, Loss:  2.272\n",
      "Epoch   0 Batch  115/1077 - Train Accuracy:  0.446, Validation Accuracy:  0.509, Loss:  2.359\n",
      "Epoch   0 Batch  116/1077 - Train Accuracy:  0.406, Validation Accuracy:  0.495, Loss:  2.317\n",
      "Epoch   0 Batch  117/1077 - Train Accuracy:  0.387, Validation Accuracy:  0.474, Loss:  2.358\n",
      "Epoch   0 Batch  118/1077 - Train Accuracy:  0.387, Validation Accuracy:  0.483, Loss:  2.360\n",
      "Epoch   0 Batch  119/1077 - Train Accuracy:  0.437, Validation Accuracy:  0.505, Loss:  2.258\n",
      "Epoch   0 Batch  120/1077 - Train Accuracy:  0.421, Validation Accuracy:  0.478, Loss:  2.279\n",
      "Epoch   0 Batch  121/1077 - Train Accuracy:  0.401, Validation Accuracy:  0.461, Loss:  2.267\n",
      "Epoch   0 Batch  122/1077 - Train Accuracy:  0.433, Validation Accuracy:  0.489, Loss:  2.250\n",
      "Epoch   0 Batch  123/1077 - Train Accuracy:  0.474, Validation Accuracy:  0.505, Loss:  2.137\n",
      "Epoch   0 Batch  124/1077 - Train Accuracy:  0.432, Validation Accuracy:  0.498, Loss:  2.232\n",
      "Epoch   0 Batch  125/1077 - Train Accuracy:  0.441, Validation Accuracy:  0.471, Loss:  2.146\n",
      "Epoch   0 Batch  126/1077 - Train Accuracy:  0.450, Validation Accuracy:  0.480, Loss:  2.130\n",
      "Epoch   0 Batch  127/1077 - Train Accuracy:  0.459, Validation Accuracy:  0.495, Loss:  2.180\n",
      "Epoch   0 Batch  128/1077 - Train Accuracy:  0.478, Validation Accuracy:  0.483, Loss:  2.098\n",
      "Epoch   0 Batch  129/1077 - Train Accuracy:  0.442, Validation Accuracy:  0.480, Loss:  2.204\n",
      "Epoch   0 Batch  130/1077 - Train Accuracy:  0.449, Validation Accuracy:  0.492, Loss:  2.083\n",
      "Epoch   0 Batch  131/1077 - Train Accuracy:  0.439, Validation Accuracy:  0.493, Loss:  2.168\n",
      "Epoch   0 Batch  132/1077 - Train Accuracy:  0.435, Validation Accuracy:  0.500, Loss:  2.160\n",
      "Epoch   0 Batch  133/1077 - Train Accuracy:  0.426, Validation Accuracy:  0.495, Loss:  2.113\n",
      "Epoch   0 Batch  134/1077 - Train Accuracy:  0.435, Validation Accuracy:  0.496, Loss:  2.052\n",
      "Epoch   0 Batch  135/1077 - Train Accuracy:  0.427, Validation Accuracy:  0.495, Loss:  2.190\n",
      "Epoch   0 Batch  136/1077 - Train Accuracy:  0.464, Validation Accuracy:  0.499, Loss:  2.114\n",
      "Epoch   0 Batch  137/1077 - Train Accuracy:  0.482, Validation Accuracy:  0.493, Loss:  1.963\n",
      "Epoch   0 Batch  138/1077 - Train Accuracy:  0.451, Validation Accuracy:  0.514, Loss:  2.046\n",
      "Epoch   0 Batch  139/1077 - Train Accuracy:  0.451, Validation Accuracy:  0.485, Loss:  2.052\n",
      "Epoch   0 Batch  140/1077 - Train Accuracy:  0.388, Validation Accuracy:  0.479, Loss:  2.153\n",
      "Epoch   0 Batch  141/1077 - Train Accuracy:  0.454, Validation Accuracy:  0.502, Loss:  2.089\n",
      "Epoch   0 Batch  142/1077 - Train Accuracy:  0.482, Validation Accuracy:  0.502, Loss:  1.948\n",
      "Epoch   0 Batch  143/1077 - Train Accuracy:  0.438, Validation Accuracy:  0.476, Loss:  2.066\n",
      "Epoch   0 Batch  144/1077 - Train Accuracy:  0.410, Validation Accuracy:  0.473, Loss:  2.170\n",
      "Epoch   0 Batch  145/1077 - Train Accuracy:  0.480, Validation Accuracy:  0.501, Loss:  1.939\n",
      "Epoch   0 Batch  146/1077 - Train Accuracy:  0.478, Validation Accuracy:  0.502, Loss:  1.952\n",
      "Epoch   0 Batch  147/1077 - Train Accuracy:  0.406, Validation Accuracy:  0.478, Loss:  2.029\n",
      "Epoch   0 Batch  148/1077 - Train Accuracy:  0.450, Validation Accuracy:  0.479, Loss:  1.965\n",
      "Epoch   0 Batch  149/1077 - Train Accuracy:  0.431, Validation Accuracy:  0.500, Loss:  2.019\n",
      "Epoch   0 Batch  150/1077 - Train Accuracy:  0.477, Validation Accuracy:  0.499, Loss:  1.917\n",
      "Epoch   0 Batch  151/1077 - Train Accuracy:  0.449, Validation Accuracy:  0.480, Loss:  1.927\n",
      "Epoch   0 Batch  152/1077 - Train Accuracy:  0.447, Validation Accuracy:  0.481, Loss:  1.973\n",
      "Epoch   0 Batch  153/1077 - Train Accuracy:  0.448, Validation Accuracy:  0.493, Loss:  2.009\n",
      "Epoch   0 Batch  154/1077 - Train Accuracy:  0.412, Validation Accuracy:  0.490, Loss:  2.042\n",
      "Epoch   0 Batch  155/1077 - Train Accuracy:  0.447, Validation Accuracy:  0.488, Loss:  1.963\n",
      "Epoch   0 Batch  156/1077 - Train Accuracy:  0.451, Validation Accuracy:  0.502, Loss:  1.888\n",
      "Epoch   0 Batch  157/1077 - Train Accuracy:  0.455, Validation Accuracy:  0.490, Loss:  1.923\n",
      "Epoch   0 Batch  158/1077 - Train Accuracy:  0.457, Validation Accuracy:  0.500, Loss:  1.936\n",
      "Epoch   0 Batch  159/1077 - Train Accuracy:  0.459, Validation Accuracy:  0.488, Loss:  1.833\n",
      "Epoch   0 Batch  160/1077 - Train Accuracy:  0.431, Validation Accuracy:  0.473, Loss:  1.895\n",
      "Epoch   0 Batch  161/1077 - Train Accuracy:  0.437, Validation Accuracy:  0.487, Loss:  1.962\n",
      "Epoch   0 Batch  162/1077 - Train Accuracy:  0.447, Validation Accuracy:  0.485, Loss:  1.935\n",
      "Epoch   0 Batch  163/1077 - Train Accuracy:  0.370, Validation Accuracy:  0.454, Loss:  1.988\n",
      "Epoch   0 Batch  164/1077 - Train Accuracy:  0.423, Validation Accuracy:  0.480, Loss:  1.952\n",
      "Epoch   0 Batch  165/1077 - Train Accuracy:  0.410, Validation Accuracy:  0.490, Loss:  1.876\n",
      "Epoch   0 Batch  166/1077 - Train Accuracy:  0.431, Validation Accuracy:  0.453, Loss:  1.863\n",
      "Epoch   0 Batch  167/1077 - Train Accuracy:  0.386, Validation Accuracy:  0.453, Loss:  1.911\n",
      "Epoch   0 Batch  168/1077 - Train Accuracy:  0.378, Validation Accuracy:  0.449, Loss:  1.917\n",
      "Epoch   0 Batch  169/1077 - Train Accuracy:  0.433, Validation Accuracy:  0.449, Loss:  1.821\n",
      "Epoch   0 Batch  170/1077 - Train Accuracy:  0.395, Validation Accuracy:  0.481, Loss:  1.886\n",
      "Epoch   0 Batch  171/1077 - Train Accuracy:  0.478, Validation Accuracy:  0.481, Loss:  1.780\n",
      "Epoch   0 Batch  172/1077 - Train Accuracy:  0.477, Validation Accuracy:  0.482, Loss:  1.762\n",
      "Epoch   0 Batch  173/1077 - Train Accuracy:  0.423, Validation Accuracy:  0.504, Loss:  2.002\n",
      "Epoch   0 Batch  174/1077 - Train Accuracy:  0.463, Validation Accuracy:  0.478, Loss:  1.779\n",
      "Epoch   0 Batch  175/1077 - Train Accuracy:  0.466, Validation Accuracy:  0.488, Loss:  1.804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch  176/1077 - Train Accuracy:  0.409, Validation Accuracy:  0.499, Loss:  1.846\n",
      "Epoch   0 Batch  177/1077 - Train Accuracy:  0.396, Validation Accuracy:  0.461, Loss:  1.906\n",
      "Epoch   0 Batch  178/1077 - Train Accuracy:  0.459, Validation Accuracy:  0.469, Loss:  1.811\n",
      "Epoch   0 Batch  179/1077 - Train Accuracy:  0.451, Validation Accuracy:  0.495, Loss:  1.837\n",
      "Epoch   0 Batch  180/1077 - Train Accuracy:  0.419, Validation Accuracy:  0.474, Loss:  1.841\n",
      "Epoch   0 Batch  181/1077 - Train Accuracy:  0.436, Validation Accuracy:  0.483, Loss:  1.768\n",
      "Epoch   0 Batch  182/1077 - Train Accuracy:  0.462, Validation Accuracy:  0.511, Loss:  1.784\n",
      "Epoch   0 Batch  183/1077 - Train Accuracy:  0.407, Validation Accuracy:  0.482, Loss:  1.828\n",
      "Epoch   0 Batch  184/1077 - Train Accuracy:  0.420, Validation Accuracy:  0.484, Loss:  1.720\n",
      "Epoch   0 Batch  185/1077 - Train Accuracy:  0.420, Validation Accuracy:  0.486, Loss:  1.747\n",
      "Epoch   0 Batch  186/1077 - Train Accuracy:  0.446, Validation Accuracy:  0.504, Loss:  1.828\n",
      "Epoch   0 Batch  187/1077 - Train Accuracy:  0.421, Validation Accuracy:  0.467, Loss:  1.795\n",
      "Epoch   0 Batch  188/1077 - Train Accuracy:  0.411, Validation Accuracy:  0.484, Loss:  1.747\n",
      "Epoch   0 Batch  189/1077 - Train Accuracy:  0.460, Validation Accuracy:  0.495, Loss:  1.807\n",
      "Epoch   0 Batch  190/1077 - Train Accuracy:  0.466, Validation Accuracy:  0.506, Loss:  1.773\n",
      "Epoch   0 Batch  191/1077 - Train Accuracy:  0.495, Validation Accuracy:  0.490, Loss:  1.624\n",
      "Epoch   0 Batch  192/1077 - Train Accuracy:  0.472, Validation Accuracy:  0.487, Loss:  1.779\n",
      "Epoch   0 Batch  193/1077 - Train Accuracy:  0.485, Validation Accuracy:  0.492, Loss:  1.718\n",
      "Epoch   0 Batch  194/1077 - Train Accuracy:  0.485, Validation Accuracy:  0.487, Loss:  1.655\n",
      "Epoch   0 Batch  195/1077 - Train Accuracy:  0.434, Validation Accuracy:  0.492, Loss:  1.792\n",
      "Epoch   0 Batch  196/1077 - Train Accuracy:  0.464, Validation Accuracy:  0.500, Loss:  1.701\n",
      "Epoch   0 Batch  197/1077 - Train Accuracy:  0.457, Validation Accuracy:  0.495, Loss:  1.744\n",
      "Epoch   0 Batch  198/1077 - Train Accuracy:  0.445, Validation Accuracy:  0.481, Loss:  1.665\n",
      "Epoch   0 Batch  199/1077 - Train Accuracy:  0.462, Validation Accuracy:  0.493, Loss:  1.679\n",
      "Epoch   0 Batch  200/1077 - Train Accuracy:  0.429, Validation Accuracy:  0.497, Loss:  1.733\n",
      "Epoch   0 Batch  201/1077 - Train Accuracy:  0.497, Validation Accuracy:  0.520, Loss:  1.640\n",
      "Epoch   0 Batch  202/1077 - Train Accuracy:  0.486, Validation Accuracy:  0.523, Loss:  1.709\n",
      "Epoch   0 Batch  203/1077 - Train Accuracy:  0.498, Validation Accuracy:  0.535, Loss:  1.665\n",
      "Epoch   0 Batch  204/1077 - Train Accuracy:  0.476, Validation Accuracy:  0.522, Loss:  1.680\n",
      "Epoch   0 Batch  205/1077 - Train Accuracy:  0.483, Validation Accuracy:  0.521, Loss:  1.744\n",
      "Epoch   0 Batch  206/1077 - Train Accuracy:  0.466, Validation Accuracy:  0.529, Loss:  1.695\n",
      "Epoch   0 Batch  207/1077 - Train Accuracy:  0.455, Validation Accuracy:  0.522, Loss:  1.749\n",
      "Epoch   0 Batch  208/1077 - Train Accuracy:  0.469, Validation Accuracy:  0.520, Loss:  1.731\n",
      "Epoch   0 Batch  209/1077 - Train Accuracy:  0.491, Validation Accuracy:  0.522, Loss:  1.556\n",
      "Epoch   0 Batch  210/1077 - Train Accuracy:  0.461, Validation Accuracy:  0.488, Loss:  1.727\n",
      "Epoch   0 Batch  211/1077 - Train Accuracy:  0.440, Validation Accuracy:  0.502, Loss:  1.668\n",
      "Epoch   0 Batch  212/1077 - Train Accuracy:  0.480, Validation Accuracy:  0.511, Loss:  1.606\n",
      "Epoch   0 Batch  213/1077 - Train Accuracy:  0.457, Validation Accuracy:  0.503, Loss:  1.659\n",
      "Epoch   0 Batch  214/1077 - Train Accuracy:  0.438, Validation Accuracy:  0.504, Loss:  1.653\n",
      "Epoch   0 Batch  215/1077 - Train Accuracy:  0.453, Validation Accuracy:  0.514, Loss:  1.618\n",
      "Epoch   0 Batch  216/1077 - Train Accuracy:  0.469, Validation Accuracy:  0.521, Loss:  1.700\n",
      "Epoch   0 Batch  217/1077 - Train Accuracy:  0.464, Validation Accuracy:  0.506, Loss:  1.587\n",
      "Epoch   0 Batch  218/1077 - Train Accuracy:  0.438, Validation Accuracy:  0.509, Loss:  1.694\n",
      "Epoch   0 Batch  219/1077 - Train Accuracy:  0.489, Validation Accuracy:  0.517, Loss:  1.643\n",
      "Epoch   0 Batch  220/1077 - Train Accuracy:  0.458, Validation Accuracy:  0.517, Loss:  1.675\n",
      "Epoch   0 Batch  221/1077 - Train Accuracy:  0.513, Validation Accuracy:  0.536, Loss:  1.627\n",
      "Epoch   0 Batch  222/1077 - Train Accuracy:  0.444, Validation Accuracy:  0.545, Loss:  1.687\n",
      "Epoch   0 Batch  223/1077 - Train Accuracy:  0.497, Validation Accuracy:  0.514, Loss:  1.549\n",
      "Epoch   0 Batch  224/1077 - Train Accuracy:  0.475, Validation Accuracy:  0.528, Loss:  1.658\n",
      "Epoch   0 Batch  225/1077 - Train Accuracy:  0.461, Validation Accuracy:  0.526, Loss:  1.628\n",
      "Epoch   0 Batch  226/1077 - Train Accuracy:  0.475, Validation Accuracy:  0.526, Loss:  1.616\n",
      "Epoch   0 Batch  227/1077 - Train Accuracy:  0.445, Validation Accuracy:  0.527, Loss:  1.724\n",
      "Epoch   0 Batch  228/1077 - Train Accuracy:  0.481, Validation Accuracy:  0.497, Loss:  1.635\n",
      "Epoch   0 Batch  229/1077 - Train Accuracy:  0.489, Validation Accuracy:  0.508, Loss:  1.668\n",
      "Epoch   0 Batch  230/1077 - Train Accuracy:  0.493, Validation Accuracy:  0.524, Loss:  1.537\n",
      "Epoch   0 Batch  231/1077 - Train Accuracy:  0.500, Validation Accuracy:  0.521, Loss:  1.502\n",
      "Epoch   0 Batch  232/1077 - Train Accuracy:  0.451, Validation Accuracy:  0.527, Loss:  1.626\n",
      "Epoch   0 Batch  233/1077 - Train Accuracy:  0.465, Validation Accuracy:  0.538, Loss:  1.658\n",
      "Epoch   0 Batch  234/1077 - Train Accuracy:  0.503, Validation Accuracy:  0.530, Loss:  1.542\n",
      "Epoch   0 Batch  235/1077 - Train Accuracy:  0.519, Validation Accuracy:  0.529, Loss:  1.534\n",
      "Epoch   0 Batch  236/1077 - Train Accuracy:  0.457, Validation Accuracy:  0.530, Loss:  1.617\n",
      "Epoch   0 Batch  237/1077 - Train Accuracy:  0.519, Validation Accuracy:  0.535, Loss:  1.468\n",
      "Epoch   0 Batch  238/1077 - Train Accuracy:  0.511, Validation Accuracy:  0.533, Loss:  1.590\n",
      "Epoch   0 Batch  239/1077 - Train Accuracy:  0.494, Validation Accuracy:  0.530, Loss:  1.512\n",
      "Epoch   0 Batch  240/1077 - Train Accuracy:  0.511, Validation Accuracy:  0.541, Loss:  1.527\n",
      "Epoch   0 Batch  241/1077 - Train Accuracy:  0.521, Validation Accuracy:  0.545, Loss:  1.483\n",
      "Epoch   0 Batch  242/1077 - Train Accuracy:  0.469, Validation Accuracy:  0.518, Loss:  1.563\n",
      "Epoch   0 Batch  243/1077 - Train Accuracy:  0.420, Validation Accuracy:  0.527, Loss:  1.608\n",
      "Epoch   0 Batch  244/1077 - Train Accuracy:  0.530, Validation Accuracy:  0.537, Loss:  1.539\n",
      "Epoch   0 Batch  245/1077 - Train Accuracy:  0.490, Validation Accuracy:  0.526, Loss:  1.455\n",
      "Epoch   0 Batch  246/1077 - Train Accuracy:  0.471, Validation Accuracy:  0.510, Loss:  1.542\n",
      "Epoch   0 Batch  247/1077 - Train Accuracy:  0.534, Validation Accuracy:  0.527, Loss:  1.475\n",
      "Epoch   0 Batch  248/1077 - Train Accuracy:  0.520, Validation Accuracy:  0.543, Loss:  1.525\n",
      "Epoch   0 Batch  249/1077 - Train Accuracy:  0.507, Validation Accuracy:  0.535, Loss:  1.525\n",
      "Epoch   0 Batch  250/1077 - Train Accuracy:  0.494, Validation Accuracy:  0.508, Loss:  1.434\n",
      "Epoch   0 Batch  251/1077 - Train Accuracy:  0.524, Validation Accuracy:  0.544, Loss:  1.555\n",
      "Epoch   0 Batch  252/1077 - Train Accuracy:  0.504, Validation Accuracy:  0.552, Loss:  1.510\n",
      "Epoch   0 Batch  253/1077 - Train Accuracy:  0.526, Validation Accuracy:  0.535, Loss:  1.451\n",
      "Epoch   0 Batch  254/1077 - Train Accuracy:  0.482, Validation Accuracy:  0.521, Loss:  1.552\n",
      "Epoch   0 Batch  255/1077 - Train Accuracy:  0.480, Validation Accuracy:  0.542, Loss:  1.573\n",
      "Epoch   0 Batch  256/1077 - Train Accuracy:  0.446, Validation Accuracy:  0.555, Loss:  1.585\n",
      "Epoch   0 Batch  257/1077 - Train Accuracy:  0.531, Validation Accuracy:  0.555, Loss:  1.463\n",
      "Epoch   0 Batch  258/1077 - Train Accuracy:  0.492, Validation Accuracy:  0.533, Loss:  1.473\n",
      "Epoch   0 Batch  259/1077 - Train Accuracy:  0.469, Validation Accuracy:  0.517, Loss:  1.476\n",
      "Epoch   0 Batch  260/1077 - Train Accuracy:  0.483, Validation Accuracy:  0.526, Loss:  1.458\n",
      "Epoch   0 Batch  261/1077 - Train Accuracy:  0.529, Validation Accuracy:  0.551, Loss:  1.417\n",
      "Epoch   0 Batch  262/1077 - Train Accuracy:  0.486, Validation Accuracy:  0.537, Loss:  1.470\n",
      "Epoch   0 Batch  263/1077 - Train Accuracy:  0.512, Validation Accuracy:  0.537, Loss:  1.475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch  264/1077 - Train Accuracy:  0.445, Validation Accuracy:  0.529, Loss:  1.492\n",
      "Epoch   0 Batch  265/1077 - Train Accuracy:  0.499, Validation Accuracy:  0.534, Loss:  1.434\n",
      "Epoch   0 Batch  266/1077 - Train Accuracy:  0.504, Validation Accuracy:  0.546, Loss:  1.422\n",
      "Epoch   0 Batch  267/1077 - Train Accuracy:  0.497, Validation Accuracy:  0.548, Loss:  1.404\n",
      "Epoch   0 Batch  268/1077 - Train Accuracy:  0.511, Validation Accuracy:  0.547, Loss:  1.441\n",
      "Epoch   0 Batch  269/1077 - Train Accuracy:  0.468, Validation Accuracy:  0.531, Loss:  1.551\n",
      "Epoch   0 Batch  270/1077 - Train Accuracy:  0.490, Validation Accuracy:  0.561, Loss:  1.495\n",
      "Epoch   0 Batch  271/1077 - Train Accuracy:  0.507, Validation Accuracy:  0.549, Loss:  1.427\n",
      "Epoch   0 Batch  272/1077 - Train Accuracy:  0.509, Validation Accuracy:  0.538, Loss:  1.434\n",
      "Epoch   0 Batch  273/1077 - Train Accuracy:  0.510, Validation Accuracy:  0.533, Loss:  1.411\n",
      "Epoch   0 Batch  274/1077 - Train Accuracy:  0.493, Validation Accuracy:  0.527, Loss:  1.456\n",
      "Epoch   0 Batch  275/1077 - Train Accuracy:  0.496, Validation Accuracy:  0.540, Loss:  1.399\n",
      "Epoch   0 Batch  276/1077 - Train Accuracy:  0.479, Validation Accuracy:  0.548, Loss:  1.460\n",
      "Epoch   0 Batch  277/1077 - Train Accuracy:  0.509, Validation Accuracy:  0.540, Loss:  1.418\n",
      "Epoch   0 Batch  278/1077 - Train Accuracy:  0.489, Validation Accuracy:  0.548, Loss:  1.492\n",
      "Epoch   0 Batch  279/1077 - Train Accuracy:  0.491, Validation Accuracy:  0.550, Loss:  1.508\n",
      "Epoch   0 Batch  280/1077 - Train Accuracy:  0.534, Validation Accuracy:  0.551, Loss:  1.466\n",
      "Epoch   0 Batch  281/1077 - Train Accuracy:  0.499, Validation Accuracy:  0.542, Loss:  1.466\n",
      "Epoch   0 Batch  282/1077 - Train Accuracy:  0.468, Validation Accuracy:  0.551, Loss:  1.527\n",
      "Epoch   0 Batch  283/1077 - Train Accuracy:  0.516, Validation Accuracy:  0.545, Loss:  1.477\n",
      "Epoch   0 Batch  284/1077 - Train Accuracy:  0.525, Validation Accuracy:  0.553, Loss:  1.476\n",
      "Epoch   0 Batch  285/1077 - Train Accuracy:  0.567, Validation Accuracy:  0.551, Loss:  1.359\n",
      "Epoch   0 Batch  286/1077 - Train Accuracy:  0.552, Validation Accuracy:  0.553, Loss:  1.348\n",
      "Epoch   0 Batch  287/1077 - Train Accuracy:  0.525, Validation Accuracy:  0.549, Loss:  1.365\n",
      "Epoch   0 Batch  288/1077 - Train Accuracy:  0.491, Validation Accuracy:  0.560, Loss:  1.477\n",
      "Epoch   0 Batch  289/1077 - Train Accuracy:  0.542, Validation Accuracy:  0.560, Loss:  1.343\n",
      "Epoch   0 Batch  290/1077 - Train Accuracy:  0.522, Validation Accuracy:  0.548, Loss:  1.436\n",
      "Epoch   0 Batch  291/1077 - Train Accuracy:  0.510, Validation Accuracy:  0.551, Loss:  1.400\n",
      "Epoch   0 Batch  292/1077 - Train Accuracy:  0.541, Validation Accuracy:  0.559, Loss:  1.374\n",
      "Epoch   0 Batch  293/1077 - Train Accuracy:  0.484, Validation Accuracy:  0.550, Loss:  1.457\n",
      "Epoch   0 Batch  294/1077 - Train Accuracy:  0.553, Validation Accuracy:  0.556, Loss:  1.301\n",
      "Epoch   0 Batch  295/1077 - Train Accuracy:  0.497, Validation Accuracy:  0.551, Loss:  1.522\n",
      "Epoch   0 Batch  296/1077 - Train Accuracy:  0.526, Validation Accuracy:  0.563, Loss:  1.362\n",
      "Epoch   0 Batch  297/1077 - Train Accuracy:  0.481, Validation Accuracy:  0.548, Loss:  1.438\n",
      "Epoch   0 Batch  298/1077 - Train Accuracy:  0.529, Validation Accuracy:  0.562, Loss:  1.478\n",
      "Epoch   0 Batch  299/1077 - Train Accuracy:  0.526, Validation Accuracy:  0.562, Loss:  1.341\n",
      "Epoch   0 Batch  300/1077 - Train Accuracy:  0.501, Validation Accuracy:  0.555, Loss:  1.422\n",
      "Epoch   0 Batch  301/1077 - Train Accuracy:  0.519, Validation Accuracy:  0.563, Loss:  1.347\n",
      "Epoch   0 Batch  302/1077 - Train Accuracy:  0.580, Validation Accuracy:  0.558, Loss:  1.378\n",
      "Epoch   0 Batch  303/1077 - Train Accuracy:  0.512, Validation Accuracy:  0.561, Loss:  1.370\n",
      "Epoch   0 Batch  304/1077 - Train Accuracy:  0.522, Validation Accuracy:  0.559, Loss:  1.321\n",
      "Epoch   0 Batch  305/1077 - Train Accuracy:  0.549, Validation Accuracy:  0.555, Loss:  1.385\n",
      "Epoch   0 Batch  306/1077 - Train Accuracy:  0.527, Validation Accuracy:  0.559, Loss:  1.371\n",
      "Epoch   0 Batch  307/1077 - Train Accuracy:  0.487, Validation Accuracy:  0.551, Loss:  1.360\n",
      "Epoch   0 Batch  308/1077 - Train Accuracy:  0.499, Validation Accuracy:  0.556, Loss:  1.418\n",
      "Epoch   0 Batch  309/1077 - Train Accuracy:  0.520, Validation Accuracy:  0.558, Loss:  1.299\n",
      "Epoch   0 Batch  310/1077 - Train Accuracy:  0.491, Validation Accuracy:  0.550, Loss:  1.378\n",
      "Epoch   0 Batch  311/1077 - Train Accuracy:  0.542, Validation Accuracy:  0.545, Loss:  1.319\n",
      "Epoch   0 Batch  312/1077 - Train Accuracy:  0.561, Validation Accuracy:  0.563, Loss:  1.433\n",
      "Epoch   0 Batch  313/1077 - Train Accuracy:  0.522, Validation Accuracy:  0.565, Loss:  1.328\n",
      "Epoch   0 Batch  314/1077 - Train Accuracy:  0.538, Validation Accuracy:  0.567, Loss:  1.339\n",
      "Epoch   0 Batch  315/1077 - Train Accuracy:  0.528, Validation Accuracy:  0.560, Loss:  1.304\n",
      "Epoch   0 Batch  316/1077 - Train Accuracy:  0.531, Validation Accuracy:  0.560, Loss:  1.288\n",
      "Epoch   0 Batch  317/1077 - Train Accuracy:  0.520, Validation Accuracy:  0.560, Loss:  1.387\n",
      "Epoch   0 Batch  318/1077 - Train Accuracy:  0.511, Validation Accuracy:  0.570, Loss:  1.319\n",
      "Epoch   0 Batch  319/1077 - Train Accuracy:  0.500, Validation Accuracy:  0.573, Loss:  1.335\n",
      "Epoch   0 Batch  320/1077 - Train Accuracy:  0.548, Validation Accuracy:  0.572, Loss:  1.332\n",
      "Epoch   0 Batch  321/1077 - Train Accuracy:  0.527, Validation Accuracy:  0.561, Loss:  1.316\n",
      "Epoch   0 Batch  322/1077 - Train Accuracy:  0.526, Validation Accuracy:  0.560, Loss:  1.259\n",
      "Epoch   0 Batch  323/1077 - Train Accuracy:  0.534, Validation Accuracy:  0.558, Loss:  1.343\n",
      "Epoch   0 Batch  324/1077 - Train Accuracy:  0.514, Validation Accuracy:  0.558, Loss:  1.344\n",
      "Epoch   0 Batch  325/1077 - Train Accuracy:  0.579, Validation Accuracy:  0.569, Loss:  1.279\n",
      "Epoch   0 Batch  326/1077 - Train Accuracy:  0.573, Validation Accuracy:  0.571, Loss:  1.234\n",
      "Epoch   0 Batch  327/1077 - Train Accuracy:  0.511, Validation Accuracy:  0.568, Loss:  1.322\n",
      "Epoch   0 Batch  328/1077 - Train Accuracy:  0.548, Validation Accuracy:  0.562, Loss:  1.345\n",
      "Epoch   0 Batch  329/1077 - Train Accuracy:  0.495, Validation Accuracy:  0.573, Loss:  1.347\n",
      "Epoch   0 Batch  330/1077 - Train Accuracy:  0.545, Validation Accuracy:  0.569, Loss:  1.353\n",
      "Epoch   0 Batch  331/1077 - Train Accuracy:  0.498, Validation Accuracy:  0.561, Loss:  1.293\n",
      "Epoch   0 Batch  332/1077 - Train Accuracy:  0.515, Validation Accuracy:  0.549, Loss:  1.224\n",
      "Epoch   0 Batch  333/1077 - Train Accuracy:  0.539, Validation Accuracy:  0.560, Loss:  1.322\n",
      "Epoch   0 Batch  334/1077 - Train Accuracy:  0.512, Validation Accuracy:  0.562, Loss:  1.368\n",
      "Epoch   0 Batch  335/1077 - Train Accuracy:  0.562, Validation Accuracy:  0.558, Loss:  1.239\n",
      "Epoch   0 Batch  336/1077 - Train Accuracy:  0.526, Validation Accuracy:  0.556, Loss:  1.310\n",
      "Epoch   0 Batch  337/1077 - Train Accuracy:  0.502, Validation Accuracy:  0.561, Loss:  1.316\n",
      "Epoch   0 Batch  338/1077 - Train Accuracy:  0.561, Validation Accuracy:  0.561, Loss:  1.362\n",
      "Epoch   0 Batch  339/1077 - Train Accuracy:  0.541, Validation Accuracy:  0.563, Loss:  1.297\n",
      "Epoch   0 Batch  340/1077 - Train Accuracy:  0.507, Validation Accuracy:  0.564, Loss:  1.316\n",
      "Epoch   0 Batch  341/1077 - Train Accuracy:  0.530, Validation Accuracy:  0.556, Loss:  1.310\n",
      "Epoch   0 Batch  342/1077 - Train Accuracy:  0.514, Validation Accuracy:  0.555, Loss:  1.290\n",
      "Epoch   0 Batch  343/1077 - Train Accuracy:  0.511, Validation Accuracy:  0.554, Loss:  1.291\n",
      "Epoch   0 Batch  344/1077 - Train Accuracy:  0.542, Validation Accuracy:  0.559, Loss:  1.326\n",
      "Epoch   0 Batch  345/1077 - Train Accuracy:  0.548, Validation Accuracy:  0.558, Loss:  1.261\n",
      "Epoch   0 Batch  346/1077 - Train Accuracy:  0.507, Validation Accuracy:  0.561, Loss:  1.317\n",
      "Epoch   0 Batch  347/1077 - Train Accuracy:  0.536, Validation Accuracy:  0.563, Loss:  1.194\n",
      "Epoch   0 Batch  348/1077 - Train Accuracy:  0.538, Validation Accuracy:  0.563, Loss:  1.235\n",
      "Epoch   0 Batch  349/1077 - Train Accuracy:  0.489, Validation Accuracy:  0.561, Loss:  1.287\n",
      "Epoch   0 Batch  350/1077 - Train Accuracy:  0.478, Validation Accuracy:  0.563, Loss:  1.306\n",
      "Epoch   0 Batch  351/1077 - Train Accuracy:  0.502, Validation Accuracy:  0.573, Loss:  1.339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch  352/1077 - Train Accuracy:  0.470, Validation Accuracy:  0.554, Loss:  1.316\n",
      "Epoch   0 Batch  353/1077 - Train Accuracy:  0.482, Validation Accuracy:  0.563, Loss:  1.311\n",
      "Epoch   0 Batch  354/1077 - Train Accuracy:  0.501, Validation Accuracy:  0.566, Loss:  1.285\n",
      "Epoch   0 Batch  355/1077 - Train Accuracy:  0.501, Validation Accuracy:  0.581, Loss:  1.264\n",
      "Epoch   0 Batch  356/1077 - Train Accuracy:  0.520, Validation Accuracy:  0.578, Loss:  1.285\n",
      "Epoch   0 Batch  357/1077 - Train Accuracy:  0.547, Validation Accuracy:  0.575, Loss:  1.247\n",
      "Epoch   0 Batch  358/1077 - Train Accuracy:  0.516, Validation Accuracy:  0.574, Loss:  1.348\n",
      "Epoch   0 Batch  359/1077 - Train Accuracy:  0.528, Validation Accuracy:  0.567, Loss:  1.267\n",
      "Epoch   0 Batch  360/1077 - Train Accuracy:  0.527, Validation Accuracy:  0.569, Loss:  1.256\n",
      "Epoch   0 Batch  361/1077 - Train Accuracy:  0.550, Validation Accuracy:  0.566, Loss:  1.286\n",
      "Epoch   0 Batch  362/1077 - Train Accuracy:  0.541, Validation Accuracy:  0.565, Loss:  1.263\n",
      "Epoch   0 Batch  363/1077 - Train Accuracy:  0.481, Validation Accuracy:  0.558, Loss:  1.235\n",
      "Epoch   0 Batch  364/1077 - Train Accuracy:  0.542, Validation Accuracy:  0.560, Loss:  1.247\n",
      "Epoch   0 Batch  365/1077 - Train Accuracy:  0.515, Validation Accuracy:  0.567, Loss:  1.289\n",
      "Epoch   0 Batch  366/1077 - Train Accuracy:  0.510, Validation Accuracy:  0.571, Loss:  1.257\n",
      "Epoch   0 Batch  367/1077 - Train Accuracy:  0.538, Validation Accuracy:  0.574, Loss:  1.182\n",
      "Epoch   0 Batch  368/1077 - Train Accuracy:  0.524, Validation Accuracy:  0.570, Loss:  1.222\n",
      "Epoch   0 Batch  369/1077 - Train Accuracy:  0.495, Validation Accuracy:  0.572, Loss:  1.254\n",
      "Epoch   0 Batch  370/1077 - Train Accuracy:  0.522, Validation Accuracy:  0.572, Loss:  1.181\n",
      "Epoch   0 Batch  371/1077 - Train Accuracy:  0.569, Validation Accuracy:  0.570, Loss:  1.232\n",
      "Epoch   0 Batch  372/1077 - Train Accuracy:  0.531, Validation Accuracy:  0.565, Loss:  1.208\n",
      "Epoch   0 Batch  373/1077 - Train Accuracy:  0.589, Validation Accuracy:  0.561, Loss:  1.174\n",
      "Epoch   0 Batch  374/1077 - Train Accuracy:  0.481, Validation Accuracy:  0.560, Loss:  1.267\n",
      "Epoch   0 Batch  375/1077 - Train Accuracy:  0.568, Validation Accuracy:  0.567, Loss:  1.166\n",
      "Epoch   0 Batch  376/1077 - Train Accuracy:  0.558, Validation Accuracy:  0.562, Loss:  1.200\n",
      "Epoch   0 Batch  377/1077 - Train Accuracy:  0.527, Validation Accuracy:  0.551, Loss:  1.266\n",
      "Epoch   0 Batch  378/1077 - Train Accuracy:  0.491, Validation Accuracy:  0.556, Loss:  1.180\n",
      "Epoch   0 Batch  379/1077 - Train Accuracy:  0.523, Validation Accuracy:  0.564, Loss:  1.276\n",
      "Epoch   0 Batch  380/1077 - Train Accuracy:  0.526, Validation Accuracy:  0.570, Loss:  1.238\n",
      "Epoch   0 Batch  381/1077 - Train Accuracy:  0.520, Validation Accuracy:  0.572, Loss:  1.242\n",
      "Epoch   0 Batch  382/1077 - Train Accuracy:  0.516, Validation Accuracy:  0.566, Loss:  1.251\n",
      "Epoch   0 Batch  383/1077 - Train Accuracy:  0.532, Validation Accuracy:  0.566, Loss:  1.223\n",
      "Epoch   0 Batch  384/1077 - Train Accuracy:  0.500, Validation Accuracy:  0.566, Loss:  1.222\n",
      "Epoch   0 Batch  385/1077 - Train Accuracy:  0.521, Validation Accuracy:  0.569, Loss:  1.230\n",
      "Epoch   0 Batch  386/1077 - Train Accuracy:  0.490, Validation Accuracy:  0.564, Loss:  1.190\n",
      "Epoch   0 Batch  387/1077 - Train Accuracy:  0.529, Validation Accuracy:  0.566, Loss:  1.181\n",
      "Epoch   0 Batch  388/1077 - Train Accuracy:  0.514, Validation Accuracy:  0.566, Loss:  1.204\n",
      "Epoch   0 Batch  389/1077 - Train Accuracy:  0.527, Validation Accuracy:  0.572, Loss:  1.245\n",
      "Epoch   0 Batch  390/1077 - Train Accuracy:  0.499, Validation Accuracy:  0.574, Loss:  1.251\n",
      "Epoch   0 Batch  391/1077 - Train Accuracy:  0.544, Validation Accuracy:  0.569, Loss:  1.178\n",
      "Epoch   0 Batch  392/1077 - Train Accuracy:  0.482, Validation Accuracy:  0.568, Loss:  1.189\n",
      "Epoch   0 Batch  393/1077 - Train Accuracy:  0.565, Validation Accuracy:  0.572, Loss:  1.167\n",
      "Epoch   0 Batch  394/1077 - Train Accuracy:  0.488, Validation Accuracy:  0.567, Loss:  1.265\n",
      "Epoch   0 Batch  395/1077 - Train Accuracy:  0.536, Validation Accuracy:  0.575, Loss:  1.186\n",
      "Epoch   0 Batch  396/1077 - Train Accuracy:  0.533, Validation Accuracy:  0.575, Loss:  1.177\n",
      "Epoch   0 Batch  397/1077 - Train Accuracy:  0.568, Validation Accuracy:  0.581, Loss:  1.172\n",
      "Epoch   0 Batch  398/1077 - Train Accuracy:  0.528, Validation Accuracy:  0.569, Loss:  1.265\n",
      "Epoch   0 Batch  399/1077 - Train Accuracy:  0.433, Validation Accuracy:  0.548, Loss:  1.248\n",
      "Epoch   0 Batch  400/1077 - Train Accuracy:  0.509, Validation Accuracy:  0.554, Loss:  1.251\n",
      "Epoch   0 Batch  401/1077 - Train Accuracy:  0.511, Validation Accuracy:  0.561, Loss:  1.251\n",
      "Epoch   0 Batch  402/1077 - Train Accuracy:  0.544, Validation Accuracy:  0.561, Loss:  1.177\n",
      "Epoch   0 Batch  403/1077 - Train Accuracy:  0.507, Validation Accuracy:  0.561, Loss:  1.216\n",
      "Epoch   0 Batch  404/1077 - Train Accuracy:  0.551, Validation Accuracy:  0.562, Loss:  1.126\n",
      "Epoch   0 Batch  405/1077 - Train Accuracy:  0.471, Validation Accuracy:  0.570, Loss:  1.235\n",
      "Epoch   0 Batch  406/1077 - Train Accuracy:  0.519, Validation Accuracy:  0.567, Loss:  1.172\n",
      "Epoch   0 Batch  407/1077 - Train Accuracy:  0.517, Validation Accuracy:  0.574, Loss:  1.284\n",
      "Epoch   0 Batch  408/1077 - Train Accuracy:  0.534, Validation Accuracy:  0.574, Loss:  1.221\n",
      "Epoch   0 Batch  409/1077 - Train Accuracy:  0.503, Validation Accuracy:  0.570, Loss:  1.228\n",
      "Epoch   0 Batch  410/1077 - Train Accuracy:  0.463, Validation Accuracy:  0.570, Loss:  1.230\n",
      "Epoch   0 Batch  411/1077 - Train Accuracy:  0.512, Validation Accuracy:  0.563, Loss:  1.156\n",
      "Epoch   0 Batch  412/1077 - Train Accuracy:  0.493, Validation Accuracy:  0.570, Loss:  1.149\n",
      "Epoch   0 Batch  413/1077 - Train Accuracy:  0.553, Validation Accuracy:  0.571, Loss:  1.172\n",
      "Epoch   0 Batch  414/1077 - Train Accuracy:  0.502, Validation Accuracy:  0.561, Loss:  1.280\n",
      "Epoch   0 Batch  415/1077 - Train Accuracy:  0.547, Validation Accuracy:  0.562, Loss:  1.148\n",
      "Epoch   0 Batch  416/1077 - Train Accuracy:  0.520, Validation Accuracy:  0.565, Loss:  1.147\n",
      "Epoch   0 Batch  417/1077 - Train Accuracy:  0.507, Validation Accuracy:  0.560, Loss:  1.203\n",
      "Epoch   0 Batch  418/1077 - Train Accuracy:  0.484, Validation Accuracy:  0.570, Loss:  1.183\n",
      "Epoch   0 Batch  419/1077 - Train Accuracy:  0.513, Validation Accuracy:  0.577, Loss:  1.177\n",
      "Epoch   0 Batch  420/1077 - Train Accuracy:  0.504, Validation Accuracy:  0.575, Loss:  1.188\n",
      "Epoch   0 Batch  421/1077 - Train Accuracy:  0.530, Validation Accuracy:  0.578, Loss:  1.241\n",
      "Epoch   0 Batch  422/1077 - Train Accuracy:  0.530, Validation Accuracy:  0.573, Loss:  1.172\n",
      "Epoch   0 Batch  423/1077 - Train Accuracy:  0.547, Validation Accuracy:  0.578, Loss:  1.152\n",
      "Epoch   0 Batch  424/1077 - Train Accuracy:  0.475, Validation Accuracy:  0.578, Loss:  1.171\n",
      "Epoch   0 Batch  425/1077 - Train Accuracy:  0.583, Validation Accuracy:  0.581, Loss:  1.167\n",
      "Epoch   0 Batch  426/1077 - Train Accuracy:  0.527, Validation Accuracy:  0.567, Loss:  1.206\n",
      "Epoch   0 Batch  427/1077 - Train Accuracy:  0.513, Validation Accuracy:  0.570, Loss:  1.162\n",
      "Epoch   0 Batch  428/1077 - Train Accuracy:  0.552, Validation Accuracy:  0.572, Loss:  1.162\n",
      "Epoch   0 Batch  429/1077 - Train Accuracy:  0.553, Validation Accuracy:  0.576, Loss:  1.195\n",
      "Epoch   0 Batch  430/1077 - Train Accuracy:  0.523, Validation Accuracy:  0.578, Loss:  1.212\n",
      "Epoch   0 Batch  431/1077 - Train Accuracy:  0.518, Validation Accuracy:  0.577, Loss:  1.197\n",
      "Epoch   0 Batch  432/1077 - Train Accuracy:  0.543, Validation Accuracy:  0.578, Loss:  1.177\n",
      "Epoch   0 Batch  433/1077 - Train Accuracy:  0.512, Validation Accuracy:  0.585, Loss:  1.177\n",
      "Epoch   0 Batch  434/1077 - Train Accuracy:  0.505, Validation Accuracy:  0.579, Loss:  1.187\n",
      "Epoch   0 Batch  435/1077 - Train Accuracy:  0.535, Validation Accuracy:  0.568, Loss:  1.229\n",
      "Epoch   0 Batch  436/1077 - Train Accuracy:  0.564, Validation Accuracy:  0.579, Loss:  1.153\n",
      "Epoch   0 Batch  437/1077 - Train Accuracy:  0.507, Validation Accuracy:  0.584, Loss:  1.184\n",
      "Epoch   0 Batch  438/1077 - Train Accuracy:  0.540, Validation Accuracy:  0.583, Loss:  1.175\n",
      "Epoch   0 Batch  439/1077 - Train Accuracy:  0.532, Validation Accuracy:  0.577, Loss:  1.216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch  440/1077 - Train Accuracy:  0.538, Validation Accuracy:  0.575, Loss:  1.209\n",
      "Epoch   0 Batch  441/1077 - Train Accuracy:  0.495, Validation Accuracy:  0.578, Loss:  1.143\n",
      "Epoch   0 Batch  442/1077 - Train Accuracy:  0.542, Validation Accuracy:  0.575, Loss:  1.149\n",
      "Epoch   0 Batch  443/1077 - Train Accuracy:  0.549, Validation Accuracy:  0.569, Loss:  1.165\n",
      "Epoch   0 Batch  444/1077 - Train Accuracy:  0.507, Validation Accuracy:  0.572, Loss:  1.210\n",
      "Epoch   0 Batch  445/1077 - Train Accuracy:  0.508, Validation Accuracy:  0.571, Loss:  1.229\n",
      "Epoch   0 Batch  446/1077 - Train Accuracy:  0.525, Validation Accuracy:  0.570, Loss:  1.083\n",
      "Epoch   0 Batch  447/1077 - Train Accuracy:  0.511, Validation Accuracy:  0.573, Loss:  1.196\n",
      "Epoch   0 Batch  448/1077 - Train Accuracy:  0.532, Validation Accuracy:  0.567, Loss:  1.220\n",
      "Epoch   0 Batch  449/1077 - Train Accuracy:  0.523, Validation Accuracy:  0.571, Loss:  1.182\n",
      "Epoch   0 Batch  450/1077 - Train Accuracy:  0.526, Validation Accuracy:  0.570, Loss:  1.156\n",
      "Epoch   0 Batch  451/1077 - Train Accuracy:  0.531, Validation Accuracy:  0.571, Loss:  1.158\n",
      "Epoch   0 Batch  452/1077 - Train Accuracy:  0.515, Validation Accuracy:  0.576, Loss:  1.151\n",
      "Epoch   0 Batch  453/1077 - Train Accuracy:  0.548, Validation Accuracy:  0.566, Loss:  1.112\n",
      "Epoch   0 Batch  454/1077 - Train Accuracy:  0.551, Validation Accuracy:  0.567, Loss:  1.154\n",
      "Epoch   0 Batch  455/1077 - Train Accuracy:  0.559, Validation Accuracy:  0.565, Loss:  1.089\n",
      "Epoch   0 Batch  456/1077 - Train Accuracy:  0.559, Validation Accuracy:  0.570, Loss:  1.141\n",
      "Epoch   0 Batch  457/1077 - Train Accuracy:  0.557, Validation Accuracy:  0.571, Loss:  1.123\n",
      "Epoch   0 Batch  458/1077 - Train Accuracy:  0.519, Validation Accuracy:  0.573, Loss:  1.166\n",
      "Epoch   0 Batch  459/1077 - Train Accuracy:  0.574, Validation Accuracy:  0.574, Loss:  1.173\n",
      "Epoch   0 Batch  460/1077 - Train Accuracy:  0.546, Validation Accuracy:  0.572, Loss:  1.157\n",
      "Epoch   0 Batch  461/1077 - Train Accuracy:  0.534, Validation Accuracy:  0.576, Loss:  1.123\n",
      "Epoch   0 Batch  462/1077 - Train Accuracy:  0.536, Validation Accuracy:  0.584, Loss:  1.183\n",
      "Epoch   0 Batch  463/1077 - Train Accuracy:  0.523, Validation Accuracy:  0.589, Loss:  1.191\n",
      "Epoch   0 Batch  464/1077 - Train Accuracy:  0.577, Validation Accuracy:  0.579, Loss:  1.151\n",
      "Epoch   0 Batch  465/1077 - Train Accuracy:  0.538, Validation Accuracy:  0.575, Loss:  1.166\n",
      "Epoch   0 Batch  466/1077 - Train Accuracy:  0.557, Validation Accuracy:  0.582, Loss:  1.127\n",
      "Epoch   0 Batch  467/1077 - Train Accuracy:  0.608, Validation Accuracy:  0.559, Loss:  1.084\n",
      "Epoch   0 Batch  468/1077 - Train Accuracy:  0.541, Validation Accuracy:  0.558, Loss:  1.163\n",
      "Epoch   0 Batch  469/1077 - Train Accuracy:  0.525, Validation Accuracy:  0.570, Loss:  1.146\n",
      "Epoch   0 Batch  470/1077 - Train Accuracy:  0.500, Validation Accuracy:  0.566, Loss:  1.187\n",
      "Epoch   0 Batch  471/1077 - Train Accuracy:  0.560, Validation Accuracy:  0.569, Loss:  1.153\n",
      "Epoch   0 Batch  472/1077 - Train Accuracy:  0.554, Validation Accuracy:  0.566, Loss:  1.135\n",
      "Epoch   0 Batch  473/1077 - Train Accuracy:  0.529, Validation Accuracy:  0.569, Loss:  1.127\n",
      "Epoch   0 Batch  474/1077 - Train Accuracy:  0.562, Validation Accuracy:  0.572, Loss:  1.131\n",
      "Epoch   0 Batch  475/1077 - Train Accuracy:  0.579, Validation Accuracy:  0.574, Loss:  1.111\n",
      "Epoch   0 Batch  476/1077 - Train Accuracy:  0.555, Validation Accuracy:  0.574, Loss:  1.184\n",
      "Epoch   0 Batch  477/1077 - Train Accuracy:  0.619, Validation Accuracy:  0.575, Loss:  1.092\n",
      "Epoch   0 Batch  478/1077 - Train Accuracy:  0.563, Validation Accuracy:  0.566, Loss:  1.169\n",
      "Epoch   0 Batch  479/1077 - Train Accuracy:  0.550, Validation Accuracy:  0.553, Loss:  1.138\n",
      "Epoch   0 Batch  480/1077 - Train Accuracy:  0.531, Validation Accuracy:  0.565, Loss:  1.131\n",
      "Epoch   0 Batch  481/1077 - Train Accuracy:  0.534, Validation Accuracy:  0.576, Loss:  1.093\n",
      "Epoch   0 Batch  482/1077 - Train Accuracy:  0.546, Validation Accuracy:  0.569, Loss:  1.201\n",
      "Epoch   0 Batch  483/1077 - Train Accuracy:  0.516, Validation Accuracy:  0.573, Loss:  1.160\n",
      "Epoch   0 Batch  484/1077 - Train Accuracy:  0.568, Validation Accuracy:  0.582, Loss:  1.130\n",
      "Epoch   0 Batch  485/1077 - Train Accuracy:  0.609, Validation Accuracy:  0.583, Loss:  1.134\n",
      "Epoch   0 Batch  486/1077 - Train Accuracy:  0.562, Validation Accuracy:  0.583, Loss:  1.137\n",
      "Epoch   0 Batch  487/1077 - Train Accuracy:  0.550, Validation Accuracy:  0.588, Loss:  1.149\n",
      "Epoch   0 Batch  488/1077 - Train Accuracy:  0.539, Validation Accuracy:  0.586, Loss:  1.142\n",
      "Epoch   0 Batch  489/1077 - Train Accuracy:  0.586, Validation Accuracy:  0.583, Loss:  1.072\n",
      "Epoch   0 Batch  490/1077 - Train Accuracy:  0.573, Validation Accuracy:  0.583, Loss:  1.103\n",
      "Epoch   0 Batch  491/1077 - Train Accuracy:  0.571, Validation Accuracy:  0.579, Loss:  1.125\n",
      "Epoch   0 Batch  492/1077 - Train Accuracy:  0.558, Validation Accuracy:  0.581, Loss:  1.223\n",
      "Epoch   0 Batch  493/1077 - Train Accuracy:  0.609, Validation Accuracy:  0.576, Loss:  1.046\n",
      "Epoch   0 Batch  494/1077 - Train Accuracy:  0.571, Validation Accuracy:  0.581, Loss:  1.079\n",
      "Epoch   0 Batch  495/1077 - Train Accuracy:  0.552, Validation Accuracy:  0.586, Loss:  1.153\n",
      "Epoch   0 Batch  496/1077 - Train Accuracy:  0.562, Validation Accuracy:  0.586, Loss:  1.118\n",
      "Epoch   0 Batch  497/1077 - Train Accuracy:  0.508, Validation Accuracy:  0.588, Loss:  1.132\n",
      "Epoch   0 Batch  498/1077 - Train Accuracy:  0.610, Validation Accuracy:  0.590, Loss:  1.120\n",
      "Epoch   0 Batch  499/1077 - Train Accuracy:  0.588, Validation Accuracy:  0.601, Loss:  1.076\n",
      "Epoch   0 Batch  500/1077 - Train Accuracy:  0.567, Validation Accuracy:  0.591, Loss:  1.113\n",
      "Epoch   0 Batch  501/1077 - Train Accuracy:  0.552, Validation Accuracy:  0.585, Loss:  1.137\n",
      "Epoch   0 Batch  502/1077 - Train Accuracy:  0.562, Validation Accuracy:  0.582, Loss:  1.126\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-b45dab1afde2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m             batch_train_logits = sess.run(\n\u001b[0;32m     47\u001b[0m                 \u001b[0minference_logits\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m                 {input_data: source_batch, keep_prob: 1.0})\n\u001b[0m\u001b[0;32m     49\u001b[0m             batch_valid_logits = sess.run(\n\u001b[0;32m     50\u001b[0m                 \u001b[0minference_logits\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import time\n",
    "\n",
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1]), (0,0)],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, np.argmax(logits, 2)))\n",
    "\n",
    "train_source = source_int_text[batch_size:]\n",
    "train_target = target_int_text[batch_size:]\n",
    "\n",
    "valid_source = helper.pad_sentence_batch(source_int_text[:batch_size])\n",
    "valid_target = helper.pad_sentence_batch(target_int_text[:batch_size])\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i, (source_batch, target_batch) in enumerate(\n",
    "                helper.batch_data(train_source, train_target, batch_size)):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: source_batch,\n",
    "                 targets: target_batch,\n",
    "                 lr: learning_rate,\n",
    "                 sequence_length: target_batch.shape[1],\n",
    "                 keep_prob: keep_probability})\n",
    "            \n",
    "            batch_train_logits = sess.run(\n",
    "                inference_logits,\n",
    "                {input_data: source_batch, keep_prob: 1.0})\n",
    "            batch_valid_logits = sess.run(\n",
    "                inference_logits,\n",
    "                {input_data: valid_source, keep_prob: 1.0})\n",
    "                \n",
    "            train_acc = get_accuracy(target_batch, batch_train_logits)\n",
    "            valid_acc = get_accuracy(np.array(valid_target), batch_valid_logits)\n",
    "            end_time = time.time()\n",
    "            print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.3f}, Validation Accuracy: {:>6.3f}, Loss: {:>6.3f}'\n",
    "                  .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存参数\n",
    "\n",
    "保存 `batch_size` 和 `save_path` 参数以进行推论（for inference）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Save parameters for checkpoint\n",
    "helper.save_params(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检查点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = helper.load_preprocess()\n",
    "load_path = helper.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 句子到序列\n",
    "\n",
    "要向模型提供要翻译的句子，你首先需要预处理该句子。实现函数 `sentence_to_seq()` 以预处理新的句子。\n",
    "\n",
    "- 将句子转换为小写形式\n",
    "- 使用 `vocab_to_int` 将单词转换为 id\n",
    " - 如果单词不在词汇表中，将其转换为`<UNK>` 单词 id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_seq(sentence, vocab_to_int):\n",
    "    \"\"\"\n",
    "    Convert a sentence to a sequence of ids\n",
    "    :param sentence: String\n",
    "    :param vocab_to_int: Dictionary to go from the words to an id\n",
    "    :return: List of word ids\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    unk = vocab_to_int['<UNK>']\n",
    "    sentence = sentence.lower()\n",
    "    sentenceid=[vocab_to_int.get(w, unk) for w in sentence.split()]\n",
    "    return sentenceid\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_sentence_to_seq(sentence_to_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 翻译\n",
    "\n",
    "将 `translate_sentence` 从英语翻译成法语。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translate_sentence = 'he saw a old yellow truck .'\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    translate_logits = sess.run(logits, {input_data: [translate_sentence], keep_prob: 1.0})[0]\n",
    "\n",
    "print('Input')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
    "print('  English Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('  Word Ids:      {}'.format([i for i in np.argmax(translate_logits, 1)]))\n",
    "print('  French Words: {}'.format([target_int_to_vocab[i] for i in np.argmax(translate_logits, 1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不完美的翻译\n",
    "\n",
    "你可能注意到了，某些句子的翻译质量比其他的要好。因为你使用的数据集只有 227 个英语单词，但实际生活中有数千个单词，只有使用这些单词的句子结果才会比较理想。对于此项目，不需要达到完美的翻译。但是，如果你想创建更好的翻译模型，则需要更好的数据。\n",
    "\n",
    "你可以使用 [WMT10 French-English corpus](http://www.statmt.org/wmt10/training-giga-fren.tar) 语料库训练模型。该数据集拥有更多的词汇，讨论的话题也更丰富。但是，训练时间要好多天的时间，所以确保你有 GPU 并且对于我们提供的数据集，你的神经网络性能很棒。提交此项目后，别忘了研究下 WMT10 语料库。\n",
    "\n",
    "\n",
    "## 提交项目\n",
    "\n",
    "提交项目时，确保先运行所有单元，然后再保存记事本。保存记事本文件为 “dlnd_language_translation.ipynb”，再通过菜单中的“文件” ->“下载为”将其另存为 HTML 格式。提交的项目文档中需包含“helper.py”和“problem_unittests.py”文件。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
